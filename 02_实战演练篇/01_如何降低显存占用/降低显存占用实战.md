# 降低显存占用

### 显存优化方法的注释

```python
"""
降低显存占用的多种方法，具体方法参考官方文档https://huggingface.co/docs/transformers/perf_train_gpu_one
1. 减少 batch size
2. 开启梯度检查点
3. 优化器选择 adafactor
4. 冻结部分层的梯度使显存占用降低
"""
```

- 注释部分列出了常见的显存优化策略，帮助在GPU上训练大模型时降低显存使用。

### 设置训练参数

```python
training_args = TrainingArguments(
    output_dir='./checkpoint',
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    metric_for_best_model="f1",
    load_best_model_at_end=True,
)
```

- `TrainingArguments`

  ：定义训练过程的参数。

  - **`output_dir='./checkpoint'`**：模型和检查点保存路径。
  - **`per_device_train_batch_size=32`**：每个设备（GPU）上的训练 batch size。
  - **`num_train_epochs=1`**：训练轮数设置为1。
  - **`evaluation_strategy="epoch"`**：每个epoch结束时进行评估。
  - **`save_strategy="epoch"`**：每个epoch结束时保存模型。
  - **`learning_rate=2e-5`**：学习率。
  - **`weight_decay=0.01`**：权重衰减。
  - **`metric_for_best_model="f1"`**：选择 F1 分数作为最佳模型的评判标准。

### 配置不同的显存优化策略

```python
training_args_2 = TrainingArguments(
    output_dir='./checkpoint',
    per_device_train_batch_size=1, # 将 batch size 设为 1
    gradient_accumulation_steps=32,
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    metric_for_best_model="f1",
    load_best_model_at_end=True,
)
```

- **`per_device_train_batch_size=1`**：将训练 batch size 设置为 1。
- **`gradient_accumulation_steps=32`**：设置梯度累积步数为 32，意味着每32个batch累积梯度后再进行一次更新，从而有效增大 batch size，减少显存占用。

### 开启梯度检查点（Gradient Checkpointing）

```python
training_args_3 = TrainingArguments(
    output_dir='./checkpoint',
    per_device_train_batch_size=1,
    gradient_accumulation_steps=32,
    gradient_checkpointing=True,  # 开启梯度检查点
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    metric_for_best_model="f1",
    load_best_model_at_end=True,
)
```

- **`gradient_checkpointing=True`**：启用梯度检查点技术，减少显存占用，但会增加计算开销。它将模型的中间激活保存到磁盘，而不是内存中。

### 使用 AdaFactor 优化器

```python
training_args_4 = TrainingArguments(
    output_dir='./checkpoint',
    per_device_train_batch_size=1,
    gradient_accumulation_steps=32,
    gradient_checkpointing=True,
    optim="adafactor",  # 修改优化器为 adafactor
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    metric_for_best_model="f1",
    load_best_model_at_end=True,
)
```

- **`optim="adafactor"`**：使用 AdaFactor 优化器，这是一种高效的优化器，能有效减少显存占用。

### 冻结模型部分层

```python
for name, param in model.bert.named_parameters():
    param.requires_grad = False
```

- **`param.requires_grad = False`**：冻结 `MacBERT` 模型的所有层的梯度，减少计算量和显存占用。冻结层后，只有未被冻结的层会进行梯度计算和优化。

### 13. 初始化 Trainer 并训练模型

```python
trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset=tokenized_datasets["train"],
                  eval_dataset=tokenized_datasets["test"],
                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
                  compute_metrics=eval_metric)
trainer.train()
```

- **`train_dataset` 和 `eval_dataset`**：分别指定训练集和测试集。
- **`data_collator=DataCollatorWithPadding(tokenizer)`**：使用分词器进行填充操作，确保所有输入序列的长度一致。
- **`compute_metrics=eval_metric`**：指定自定义评估指标函数 `eval_metric`。

### 总结

这段代码涉及加载数据集、处理文本数据、加载预训练模型、定义训练过程、评估指标和多种显存优化策略。显存优化方面通过调整 `batch size`、启用梯度检查点、选择优化器和冻结部分模型层来降低内存消耗。