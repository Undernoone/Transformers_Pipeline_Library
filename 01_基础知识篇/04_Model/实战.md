# 实战

这段代码实现了一个简单的文本分类任务，具体来说，它使用了Hugging Face的`transformers`库和PyTorch来对中文文本数据进行情感分析（正向或负向）。以下是代码的详细解释：

### 1. 导入必要的库

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import pandas as pd
from torch.utils.data import DataLoader, Dataset, random_split
```

- `torch`：用于深度学习框架，主要用于构建和训练模型。
- `transformers`：Hugging Face库，提供了各种预训练模型及其tokenizer（分词器）。
- `pandas`：用于数据处理，读取CSV文件并进行操作。
- `DataLoader`、`Dataset`、`random_split`：PyTorch中的数据处理工具，用于处理数据集和批量加载数据。

### 2. 自定义数据集类 `ClassificationDataset`

```python
class ClassificationDataset(Dataset):
    def __init__(self) -> None:
        super().__init__()
        self.data = pd.read_csv('./datasets/ChnSentiCorp_htl_all.csv')
        self.data = self.data.dropna() # 去掉空值
```

- `ClassificationDataset`继承自`Dataset`，用于从CSV文件加载数据并返回文本和标签。
- `self.data = pd.read_csv('./datasets/ChnSentiCorp_htl_all.csv')`：加载存储评论数据的CSV文件。
- `self.data = self.data.dropna()`：删除任何包含缺失值的行。

```python
def __getitem__(self, index):
        return self.data.iloc[index]["review"], self.data.iloc[index]["label"]
```

- `__getitem__`：根据索引返回对应的文本和标签，`review`是评论，`label`是标签（0表示负面评论，1表示正面评论）。

```python
def __len__(self):
        return len(self.data)
```

- `__len__`：返回数据集的大小。

### 3. 初始化Tokenizer和模型

```python
tokenizer = AutoTokenizer.from_pretrained("./rbt3")
model = AutoModelForSequenceClassification.from_pretrained("./rbt3").cuda()
```

- `tokenizer`：加载了预训练的Tokenizer `rbt3`（一个中文的预训练BERT模型）用于将文本转化为模型可以理解的输入格式。
- `model`：加载了一个预训练的序列分类模型 `rbt3`，并将其移动到GPU上（`.cuda()`）。

### 4. 数据集划分

```python
trainset, valset = random_split(ClassificationDataset(), lengths=[0.8, 0.2])
print(len(trainset), len(valset))
```

- 使用`random_split`将数据集按80%训练集和20%验证集的比例划分。

### 5. 定义数据加载器

```python
def collate_fn(batch):
    texts, labels = [], []
    for item in batch:
        texts.append(item[0])
        labels.append(item[1])
    inputs = tokenizer(texts, max_length=128, padding="max_length", truncation=True, return_tensors="pt")
    inputs["labels"] = torch.tensor(labels)
    return inputs
```

`collate_fn`：该函数对每个批次的数据进行预处理，将文本转换为模型的输入格式，并返回字典，其中包含了tokenized的文本和对应的标签。

- `max_length=128`：设置文本的最大长度为128。
- `padding="max_length"`：将所有文本填充到最大长度。
- `truncation=True`：截断超过最大长度的文本。
- `return_tensors="pt"`：返回PyTorch张量。

```python
trainset_loader = DataLoader(trainset, batch_size=32, shuffle=False, collate_fn=collate_fn)
valset_loader = DataLoader(valset, batch_size=32, shuffle=False, collate_fn=collate_fn)
```

- `trainset_loader`和`valset_loader`：为训练集和验证集创建DataLoader，批次大小为32。

### 6. 定义优化器和训练函数

```python
python
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
```

- 使用`AdamW`优化器，学习率设置为`2e-5`。

```python
def train(epoch=3, log_steps=100):
    global_step = 0
    for epoch in range(epoch):
        for batch in trainset_loader:
            if torch.cuda.is_available():
                batch = {k: v.cuda() for k, v in batch.items()}
            optimizer.zero_grad()
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            if global_step % log_steps == 0:
                print(f"Epoch: {epoch}, Global Step: {global_step}, Loss: {loss.item()}")
            global_step += 1
```

- ```python
  train
  ```

  函数：用来训练模型。

  - 每个epoch会遍历所有批次的数据。
  - `optimizer.zero_grad()`：清除之前的梯度。
  - `outputs = model(**batch)`：将输入传入模型，获取输出。
  - `loss = outputs.loss`：获取损失。
  - `loss.backward()`：计算梯度。
  - `optimizer.step()`：优化模型参数。

### 7. 定义评估函数

```python
def evaluate(loader):
    model.eval()
    total_loss = 0
    total_correct = 0
    with torch.no_grad():
        for batch in loader:
            if torch.cuda.is_available():
                batch = {k: v.cuda() for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_loss += loss.item()
            predictions = torch.argmax(outputs.logits, dim=-1)
            total_correct += (predictions == batch["labels"]).sum().item()
    return total_loss / len(loader), total_correct / len(loader.dataset)
```

- ```
  evaluate
  ```

  函数：用于评估模型在验证集上的表现。

  - `model.eval()`：将模型设置为评估模式，禁用dropout等。
  - `torch.no_grad()`：禁用梯度计算，节省内存。
  - `predictions = torch.argmax(outputs.logits, dim=-1)`：获取预测类别。

### 8. 模型训练和预测

```python
train(epoch=3, log_steps=100)
```

- 调用`train`函数进行训练，训练3个epoch。

```python
sentence = "这家餐厅的服务态度非常好，菜品也很新鲜。"
with torch.inference_mode():
    inputs = tokenizer(sentence, return_tensors="pt")
    inputs = {k: v.cuda() for k, v in inputs.items()}
    logits = model(**inputs).logits
    prediction = torch.argmax(logits, dim=-1).item()
    print(f"Sentence: {sentence}, Prediction: {prediction}")
```

- 对给定的中文句子进行预测。
- `model(**inputs).logits`：获取模型的logits。
- `torch.argmax(logits, dim=-1)`：选择最大的logit作为预测结果。

### 9. 使用Hugging Face的Pipeline

```python
model.config.id2label = {0: "差评", 1: "好评"}
pipe_text_classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
pipe_text_classifier(sentence)
```

- 设置`model.config.id2label`映射预测标签（0表示“差评”，1表示“好评”）。
- 使用Hugging Face的`pipeline`封装训练好的模型，并进行推断。根据输入的句子返回预测的分类结果。